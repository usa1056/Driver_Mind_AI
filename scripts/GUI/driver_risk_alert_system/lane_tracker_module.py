from ultralytics import YOLO
import cv2
import numpy as np
import os
import sys
import time
from collections import defaultdict, deque
import gc # å°å…¥ gc æ¨¡çµ„ï¼Œç”¨æ–¼åƒåœ¾å›æ”¶

# ç‚ºäº†æ”¯æ´ risk_modules å°å…¥
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from risk_modules.risk_analyzer import *
from risk_modules.Land_detection import *
from risk_modules.warning_controller import *
import yaml

# å°å…¥èªéŸ³è¼¸å‡ºæ¨¡çµ„
from speech_alert_system import generate_and_play_audio


class LaneTracker:
    def __init__(self, shared_alert):
        self.shared_alert = shared_alert
        # æ­·å²æ•¸æ“šéšŠåˆ—é•·åº¦ä¿æŒä¸è®Šï¼Œå› ç‚ºå®ƒå€‘é€šå¸¸ä½”ç”¨è¨˜æ†¶é«”è¼ƒå°‘
        self.object_history = defaultdict(lambda: deque(maxlen=5))
        self.risk_score_history = defaultdict(lambda: deque(maxlen=10))

        self.current_dir = os.path.dirname(os.path.abspath(__file__))
        self.yaml_path = os.path.join(self.current_dir, 'risk_modules', 'risk_params.yaml')

        with open(self.yaml_path, 'r', encoding='utf-8') as file:
            self.risk_config = yaml.safe_load(file)['risk_params']

        self.flow_roi_top = self.risk_config['optical_flow']['roi_top_ratio']
        self.flow_roi_bottom = self.risk_config['optical_flow']['roi_bottom_ratio']

        model_path = os.path.join(self.current_dir, "weight", "best2.pt")
        # åˆå§‹åŒ– YOLO æ¨¡å‹ï¼Œè€ƒæ…®åœ¨éœ€è¦æ™‚èª¿æ•´æ¨æ–·è¨­å‚™ (device)
        self.model = YOLO(model_path) 
            
        # å¯ä»¥è€ƒæ…®å°‡æ¨¡å‹åŠ è¼‰åˆ° GPU å¦‚æœæœ‰ä¸¦ä¸”è¨˜æ†¶é«”è¶³å¤ ï¼š
        # self.model = YOLO(model_path).to('cuda') 

        self.video_path = os.path.join(self.current_dir, "assets", "videoplayback.mp4")

        # æ­¤è®Šæ•¸èˆ‡è¨˜æ†¶é«”ç„¡ç›´æ¥é—œä¿‚ï¼Œä¿ç•™
        self.red_alert_active = False

    def estimate_self_speed(self, prev_gray, curr_gray):
        h, w = curr_gray.shape
        top = int(h * self.flow_roi_top)
        bottom = int(h * self.flow_roi_bottom)
        # æ³¨æ„ï¼šæ­¤è™•å‰µå»ºçš„ flow å’Œ mag é™£åˆ—ï¼Œå¦‚æœéå¸¸å¤§ï¼Œä¹Ÿå¯èƒ½ä½”ç”¨è¨˜æ†¶é«”
        # ä½†é€šå¸¸å…‰æµè¨ˆç®—çš„ ROI è¼ƒå°ï¼Œå½±éŸ¿æœ‰é™
        flow = cv2.calcOpticalFlowFarneback(prev_gray[top:bottom], curr_gray[top:bottom],
                                            None, 0.5, 3, 15, 3, 5, 1.2, 0)
        mag = np.linalg.norm(flow, axis=2)
        
        # é¡¯å¼åˆªé™¤ä¸å†éœ€è¦çš„å±€éƒ¨è®Šæ•¸
        del flow
        return np.mean(mag)

    def start(self):
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # ç²å–åŸå§‹å¹€çš„å¯¬é«˜
        original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # å®šç¾©è™•ç†å¹€çš„ç›®æ¨™å°ºå¯¸ï¼Œé¡¯è‘—é™ä½è¨˜æ†¶é«”æ¶ˆè€—
        # YOLOv8 æ¨¡å‹ imgsz=320ï¼Œæ‰€ä»¥è¼¸å…¥çµ¦æ¨¡å‹çš„åœ–åƒæœƒè¢«ç¸®æ”¾åˆ° 320x320ã€‚
        # ä½†å¦‚æœåŸå§‹å¹€å¾ˆå¤§ï¼Œç¸®æ”¾éç¨‹ä¹Ÿå¯èƒ½è€—è²»è¨˜æ†¶é«”ã€‚
        # é€™è£¡è¨­ç½®ä¸€å€‹ä¸­é–“å°ºå¯¸ï¼Œå¯ä»¥æ¸›å°‘æ•´é«”è¨˜æ†¶é«”è² æ“”ã€‚
        # ä¿æŒé•·å¯¬æ¯”ï¼Œä¾‹å¦‚å°‡å¯¬åº¦å›ºå®šç‚º 640 æˆ– 480ï¼Œä¸¦è¨ˆç®—ç›¸æ‡‰çš„é«˜åº¦ã€‚
        target_width = 640 # å¯ä»¥å˜—è©¦ 480, 320 ç­‰æ›´å°çš„å€¼
        target_height = int(original_height * (target_width / original_width))
        
        print(f"Original Frame Size: {original_width}x{original_height}")
        print(f"Processing Frame Size: {target_width}x{target_height}")

        frame_skip = 5
        frame_idx = 0
        prev_smoothed_speed = 0
        gray_history = deque(maxlen=3) # æ­·å²ç°åº¦å¹€ï¼Œä¿æŒå°‘é‡
        speed_fail_count = 0
        MAX_FAIL_COUNT = 3

        # èª¿æ•´ VideoWriter çš„è¼¸å‡ºå°ºå¯¸ç‚ºè™•ç†å¾Œçš„å°ºå¯¸
        out = cv2.VideoWriter("demo.mp4",
                              cv2.VideoWriter_fourcc(*"mp4v"),
                              int(fps // frame_skip),
                              (target_width, target_height)) # è¼¸å‡ºå°ºå¯¸èˆ‡è™•ç†å°ºå¯¸ä¸€è‡´

        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                frame_start_time = time.time()
                frame_idx += 1
                if frame_idx % frame_skip != 0:
                    # åœ¨è·³éå¹€æ™‚ï¼Œé¡¯å¼é‡‹æ”¾ç•¶å‰å¹€çš„è¨˜æ†¶é«”
                    del frame 
                    continue

                # å°‡è®€å–çš„åŸå§‹å¹€ç¸®å°åˆ°ç›®æ¨™å°ºå¯¸
                processed_frame = cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_AREA)
                del frame # é‡‹æ”¾åŸå§‹å¤§å¹€çš„è¨˜æ†¶é«”

                try:
                    # process_frame å¯èƒ½æœƒå‰µå»ºæ–°çš„åœ–åƒï¼Œæ³¨æ„å…¶è¨˜æ†¶é«”ä½¿ç”¨
                    # å¦‚æœ process_frame å…§éƒ¨ä¹Ÿè™•ç†äº†ç¸®æ”¾ï¼Œé€™è£¡å¯ä»¥èª¿æ•´
                    result = process_frame(processed_frame) # ä½¿ç”¨ç¸®å°å¾Œçš„å¹€
                    # process_frame è¿”å›çš„ result ä¸­å¯èƒ½åŒ…å«æ–°çš„åœ–åƒæ•¸æ“š
                    processed_frame, _, scene_valid, left_line, right_line = result 
                    # å‡è¨­ result[0] æ˜¯è™•ç†å¾Œçš„å¹€ï¼Œå¦‚æœå®ƒè¤‡è£½äº†æ•¸æ“šï¼Œè€ƒæ…®æ¸›å°‘è¤‡è£½
                    # åœ¨é€™è£¡ï¼Œprocessed_frame è®Šæˆäº†çµæœå¹€ï¼Œæ‰€ä»¥åŸå§‹ processed_frame çš„å¼•ç”¨æœƒè¢«æ›¿æ›ï¼ŒèˆŠæ•¸æ“šæœƒè¢«GC
                    
                    if not scene_valid:
                        # åœ¨è·³éæ™‚ç¢ºä¿é‡‹æ”¾è™•ç†éçš„å¹€
                        del processed_frame
                        continue

                    curr_gray = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2GRAY)
                    gray_history.append(curr_gray) # curr_gray ä¹Ÿæ˜¯ç¸®å°å¾Œçš„å°ºå¯¸

                    if len(gray_history) >= 2:
                        speeds = []
                        for i in range(len(gray_history) - 1):
                            try:
                                s = self.estimate_self_speed(gray_history[i], gray_history[i + 1])
                                speeds.append(s)
                            except:
                                pass
                        raw_speed = np.mean(speeds) if speeds else prev_smoothed_speed
                        speed_fail_count = 0 if speeds else speed_fail_count + 1
                    else:
                        raw_speed = prev_smoothed_speed
                        speed_fail_count += 1

                    alpha = 0.3
                    speed = alpha * raw_speed + (1 - alpha) * prev_smoothed_speed

                    if speed < 0.05:
                        speed = prev_smoothed_speed
                    if speed_fail_count >= MAX_FAIL_COUNT:
                        speed *= 0.5

                    prev_smoothed_speed = speed
                    # roi_dict å’Œ scale éƒ½æ˜¯è¨ˆç®—çµæœï¼Œé€šå¸¸è¨˜æ†¶é«”ä½”ç”¨ä¸å¤§
                    roi_dict, scale = get_lane_roi_dynamic(left_line, right_line, processed_frame.shape, speed=speed)

                except Exception as e:
                    print(f"[âŒ Speed block error] {e}")
                    # åœ¨éŒ¯èª¤ç™¼ç”Ÿæ™‚ä¹Ÿå˜—è©¦é‡‹æ”¾å¯èƒ½çš„è¨˜æ†¶é«”
                    if 'processed_frame' in locals() and processed_frame is not None:
                        del processed_frame
                    continue

                # YOLO æ¨¡å‹æ¨æ–·ï¼Œè¼¸å…¥å°ºå¯¸ imgsz=320 æ˜¯æ¯”è¼ƒå°çš„ï¼Œæœ‰åŠ©æ–¼æ§åˆ¶è¨˜æ†¶é«”
                # ä½†æ˜¯ results[0].boxes.data.cpu().numpy() æœƒå°‡çµæœè¤‡è£½åˆ° CPU è¨˜æ†¶é«”
                results = self.model.track(source=processed_frame, imgsz=320, persist=True, show=False, stream=False)
                
                # é¡¯å¼åˆªé™¤ processed_frameï¼Œå› ç‚º YOLO æ¨æ–·é€šå¸¸æœƒå…§éƒ¨è¤‡è£½
                # æˆ–è€…åœ¨è™•ç†å®Œ results å¾Œå†åˆªé™¤ï¼Œå–æ±ºæ–¼ draw_risk_overlay æ˜¯å¦éœ€è¦ processed_frame
                # é€™è£¡ï¼Œdraw_risk_overlay æ¥æ”¶ annotated_frameï¼Œæ‰€ä»¥ processed_frame å¯ä»¥åœ¨æ­¤è™•åˆªé™¤
                del processed_frame 


                risky_objects = []
                seen_ids = set()
                
                is_any_red_risk_active = False 

                # éæ­·çµæœä¸¦è™•ç†é¢¨éšªç‰©ä»¶
                for r in results[0].boxes.data.cpu().numpy():
                    if len(r) < 7:
                        continue

                    track_id = int(r[6])
                    if track_id in seen_ids:
                        continue
                    seen_ids.add(track_id)

                    x1, y1, x2, y2 = map(int, r[:4])
                    center = get_center((x1, y1, x2, y2))
                    self.object_history[track_id].append(center)
                    speed, is_jump, smoothed_center, vx = compute_speed(track_id, center, self.object_history, fps=fps)

                    roi_level = get_roi_level_bbox((x1, y1, x2, y2), roi_dict)
                    if roi_level is None:
                        continue

                    score, level, stay = analyze_risk(track_id, smoothed_center, roi_level, speed, is_jump, vx)
                    self.risk_score_history[track_id].append(score)
                    smoothed_score = np.mean(self.risk_score_history[track_id])

                    if smoothed_score > self.risk_config['score_threshold']['high']:
                        level = "high"
                    elif smoothed_score > self.risk_config['score_threshold']['mid']:
                        level = "mid"
                    else:
                        level = "low"

                    risky_objects.append((x1, y1, x2, y2, track_id, smoothed_score, level))

                    if level == "mid": 
                        print(f"âš ï¸ æé†’è§¸ç™¼ï¼ID={track_id}, Level={level}, Score={smoothed_score:.2f}")
                        generate_and_play_audio("è·é›¢æœ‰é»è¿‘äº†ï¼Œå»ºè­°æ‚¨æ”¾æ…¢é€Ÿåº¦", "risk_side_alert", cooldown_seconds=5)
                        
                    if level == "high":
                        is_any_red_risk_active = True 

                # åœ¨æ‰€æœ‰ç‰©ä»¶è™•ç†å®Œç•¢å¾Œï¼Œæ ¹æ“š is_any_red_risk_active ä¾†æ§åˆ¶ç´…è‰²è­¦å ±çš„èªéŸ³
                if is_any_red_risk_active:
                    generate_and_play_audio("å·²é€²å…¥å±éšªç¯„åœï¼Œè«‹ç«‹å³æ¸›é€Ÿ", "risk_high_alert", cooldown_seconds=1.5) 
                    self.red_alert_active = True 
                else:
                    self.red_alert_active = False 

                annotated_frame = results[0].plot() # YOLO çš„ plot() æœƒè¿”å›ä¸€å€‹æ–°çš„åœ–åƒ
                del results # è™•ç†å®Œ results å¾Œé¡¯å¼åˆªé™¤ï¼Œé‡‹æ”¾è¨˜æ†¶é«”

                draw_risk_overlay(annotated_frame, risky_objects, roi_dict) 

                cv2.putText(annotated_frame, f"ROI Scale: {scale:.3f}", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)
                cv2.putText(annotated_frame, f"Speed: {speed:.2f}", (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)

                if self.shared_alert[0]:
                    cv2.putText(annotated_frame, "DROWSINESS ALERT!", (15, 140), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)
                    generate_and_play_audio("åµæ¸¬åˆ°ä½ ç–²å‹äº†ï¼Œè«‹ä¿æŒæ¸…é†’æˆ–ç¨ä½œä¼‘æ¯", "drowsiness_alert", cooldown_seconds=5)

                frame_time = time.time() - frame_start_time
                fps = 1.0 / frame_time
                cv2.putText(annotated_frame, f"FPS: {fps:.2f}", (15, 105), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)

                out.write(annotated_frame)
                cv2.imshow("Tracked Video", annotated_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                
                # æ¯æ¬¡å¾ªç’°çµæŸæ™‚ï¼Œå˜—è©¦å¼·åˆ¶åƒåœ¾å›æ”¶
                del annotated_frame # é‡‹æ”¾é¡¯ç¤ºå’Œå¯«å…¥å¾Œçš„å¹€
                gc.collect() # å˜—è©¦å¼·åˆ¶åƒåœ¾å›æ”¶

        except KeyboardInterrupt:
            print("\n[ğŸ›‘ ä½¿ç”¨è€…ä¸­æ–· Ctrl+C]")

        finally:
            cap.release()
            out.release()
            cv2.destroyAllWindows()
            gc.collect() # ç¨‹å¼çµæŸå‰å†æ¬¡é€²è¡Œåƒåœ¾å›æ”¶